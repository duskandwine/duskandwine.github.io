[{"content":" What deep learning is good for **Problems with long lists of rules****一when the traditional approach fails, machine learning/deep learning may help. **Continually changing environments****一deep learning canadapt (‘learn’) to new scenarios. **Discovering insights within large collections of data****一can you imagine trying to hand-craft rules for what 101 different kinds of food look like? What deep learning is not good for When you need explainability一the patterns learned by a deep learning model are typically uninterpretable by a human. When the traditional approach is a better option一if you can accomplish what you need with a simple rule-based system. When errors are unacceptable一since the outputs of deeplearning model aren\u0026rsquo;t always predictable. When you don\u0026rsquo;t have much data一deep learning models usually require a fairly large amount of data to produce great resulits. Machine Learning VS. Deep Learning Machine learning(more suited for structured data):\nRandom forest Gradient boosted models Naive Bayes Nearest neighbour Support vector machine \u0026hellip;many more Deep learning(more suited for unstructured data):\nNeural networks Fully connected neural network Convolutional neural network Recurrent neuralnetwork Transtormer \u0026hellip;many more What are neural networks? Types of Learning Supervised Learning Unsupervised \u0026amp; Self -supervised Learning Transfer Learning What is deep learning actually used for? Recommendation Translation, Speech recognition Computer Vision, NLP What is the pytorch Most popular research deep learning framework Write fast deep learning code in Python (able to run on a GPU/many GPUs) Able to access many pre-built deep learning models(Torch Hub, torchvision.models) Whole stack: preprocess data, model data, deploy model in your application/cloud Originally designed and Used in-house by Facebook/Meta (now open- source and Used by companies SUch as Tesla, Microsoft, OpenAl) What is a tensor？ Go google hahahaha\n","date":"2024-03-08T09:14:53+11:00","permalink":"https://duskandwine.github.io/p/pytorch_fundamental_1/","title":"PyTorch_Fundamental_1"},{"content":" ","date":"2023-10-06T18:30:49+11:00","permalink":"https://duskandwine.github.io/p/fundamentals_of_html/css/","title":"Fundamentals_of_HTML/CSS"},{"content":" 排序 快排 归并排序 二分 整数 浮点数 学习要点 主要思想 背过 思想理解，快速默写(重要) 用题目检验背过 重复刷题3-5遍 快速排序\u0026mdash;分治 确定分界点q[l]，q[(lr)/2]，q[r]，随机 ☆调整区间：使得小于分界点的数在左边，大于的在右边 递归处理左右两段 方法一(暴力) a[], b[] q[l~r] q[i] ≤ x x→a[] q[i] ≥ x x→b[] a[] → q[], b[] → q[] 方法二(优雅) 用两个指针指向头和尾，两个指针向中间移动，左边的指针遇到比分界点小的数则向右移动一位，直到遇到比临界点大的数停止移动，右边的指针则相反，当两个指针都停下时则交换两个指针指向的数，交换之后向指针中间移动一位。 快排模板 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #include \u0026lt;iostream\u0026gt; using namespace std; const int N = 1e6 +10; int n; int q[N]; void quick_sort(int q[], int l, int r) { if (l \u0026gt;= r) return; //如果区间没有数或只有一个数就无需排序了 int i = l - 1, j = r + 1, x = q[l + r \u0026gt;\u0026gt; 1]; //i，j为两个指针，x是分界点，这里的指针之所以往两边一位，是因为后面的写法中指针首先是要向中间移动一位。 while (i \u0026lt; j) { do i ++ ; while (q[i] \u0026lt; x); //先移动一次i，如果q[i]小于分界点再继续移动 do j -- ; while (q[j] \u0026gt; x); //先移动一次j，如果q[i]小于分界点再继续移动 if (i \u0026lt; j) swap(q[i], q[j]); //如果两个指针还没有相遇，就把他们的数交换 } quick_sort(q, l, j), quick_sort(q, j + 1, r); //递归处理左右两边 } int main() { scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i = 0; i \u0026lt; n; i ++) scanf(\u0026#34;%d\u0026#34;, \u0026amp;q[i]); quick_sort(q, 0 ,n -1); for (int i = 0; i \u0026lt; n; i ++) printf(\u0026#34;%d\u0026#34;, q[i]); return 0; } 归并排序\u0026mdash;分治 找中点作为分界点：mid=(l+r)/2 递归排序左边和右边 ☆归并，合二为一 用两个指针分别指向两个有序队列的开头，再定义一个新的数组记录答案，此时两个指针所指的数分别都是它们当前所在区间最小的数，比较这两个数的大小，较小的数就是原数组中最小的数，此时把这个数先放入新的数组，并把该指针向后移动一位，再次比较大小，重复以上操作直至一个指针先到达末尾，另一个指针之后所剩的数直接按序插入新数组末尾。 一个排序算法是稳定的是指，在排完序之后，原序列的两个相同数的位置不发生改变。 代码模板,双指针算法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 #include \u0026lt;iostream\u0026gt; using namespace std; const int N = 100010; int n; int q[N], tmp[N]; void merge_sort(int q[], int l, int r) { if (l \u0026gt;= r) return; int mid = l + r \u0026gt;\u0026gt; 1; //分界点 merge_sort(q, l, mid); //递归排序左边 merge_sort(q, mid + 1, r); //递归排序右边 int k = 0, i = l, j = mid + 1; //i,j是两个指针，k表示tmp中已经存在多少数 while (i \u0026lt;= mid \u0026amp;\u0026amp; j \u0026lt;= r) //i小于左半边的边界，j小于右半边的边界 if (q[i] \u0026lt;= q[j]) tmp[k ++ ] = q[i ++ ]; //把较小的数放入tmp。++是先赋值再自增 else tmp[k ++ ] = q[j ++ ]; while (i \u0026lt;= mid) tmp[k ++ ] = q[i ++ ]; //循环之后如果有剩下的数把两边有剩下的数再放入tmp while (j \u0026lt;= r) tmp[k ++ ] = q[j ++ ]; for (i = l, j = 0; i \u0026lt;= r; i ++, j ++ ) q[i] = tmp[j]; //把tmp中的数在放回q } int main() { scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i = 0; i \u0026lt; n; i ++) scanf(\u0026#34;%d\u0026#34;, \u0026amp;q[i]); merge_sort(q, 0, n-1); for (int i = 0; i \u0026lt; n; i++) printf(\u0026#34;%d \u0026#34;, q[i]); return 0; } 二分排序 整数二分：有单调性的话一定可以二分，没有单调性的话有可能可以二分。二分的本质是在区间上定义了某种性质，该性质在某一半边上满足，在另一半上不满足，使得把区间一分为二，二分就可以寻找这个性质的边界。 mid = (l + r + 1)/2,if(check(mid)),这里mid+1是因为c++向下取整，不+1可能会死循环 ture [mid, r], l = mid; false [l, mid-1], r = mid - 1. mid = (l + r)/2,if(check(mid)) ture [l, mid], r = mid; false [mid + 1, r], l = mid + 1. 代码模板 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 //区间[l, r]被划分成[l， mid]和[mid + 1, r]时使用： int bsearch_1(int l, int r) { while (l \u0026lt; r) { int mid = l + r \u0026gt;\u0026gt; 1; if (check(mid)) r = mid; else l = mid + 1; } return l; } //区间[l, r]被划分成[l, mid - 1]和[mid, r]时使用： int bsearch_2(int l, int r) { while (l \u0026lt; r) { int mid = l + r + 1 \u0026gt;\u0026gt; 1; if (check(mid)) l = mid; else r = mid - 1; } return l; } 浮点数二分 1 2 3 4 5 6 7 8 9 10 11 12 13 bool check(double x) {/* ... */} // 检查x是否满足某种性质 double bsearch_3(double l, double r) { const double eps = 1e-6; // eps 表示精度，取决于题目对精度的要求，一般比要求的精度高两位 while (r - l \u0026gt; eps) { double mid = (l + r) / 2; if (check(mid)) r = mid; else l = mid; } return l; } ","date":"2022-11-08T09:55:30+08:00","permalink":"https://duskandwine.github.io/p/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%E4%B8%80/","title":"基础算法(一)"},{"content":"总结一下上课用到的命令和一些笔记 django-admin startproject acapp #创建项目\nag \u0026ldquo;xxx\u0026rdquo; #快速查找xxx内容\npython3 manage.py runserver 0.0.0.0:8000 #django启动项目\n.gitigonre */xxxxx #不需要git的内容\npython3 manage.py startapp Name #创建app\napp中结构 |\u0026ndash; game | |\u0026ndash; init.py | |\u0026ndash; admin.py #存储管理员页面相关 | |\u0026ndash; apps.py | |\u0026ndash; migrations #系统生成 | | \u0026ndash; init.py | |\u0026ndash; models.py #定义网站中的数据库和表 | |\u0026ndash; tests.py | `\u0026ndash; views.py #写函数的\npython3 manage.py migrate #警告信息中是python manage.py migrate 应加上3， 意为同步数据库的修改 (执行之后可以打开管理员页面\npython3 manage.py createsuperuser #创建管理员用户\ntouch urls.py \u0026amp; mkdir templates #models(存放数据结构),views(存储视图/函数)以及这两个是日后主要的操作对象，templates存放的是网页模板，urls是路由\nvim game/views.py\n1 2 3 4 5 6 7 from django.http import HttpResponse def index(resquest): def index(resquest): line1 = \u0026#39;\u0026lt;h1 style=\u0026#34;text-align: center\u0026#34;\u0026gt;xxx\u0026lt;/h1\u0026gt;\u0026#39; line2 = \u0026#39;\u0026lt;img src = \u0026#34;#图片地址\u0026#34;\u0026gt;\u0026#39; return HttpResponse(line1+line2) vim game/urls.py\n1 2 3 4 5 6 from django.urls import path from game.views import index urlpatterns = [ path(\u0026#34;\u0026#34;, index, name=\u0026#34;index\u0026#34;), ] vim/acapp/acapp/urls.py\n1 2 from django.urls import path, include #修改第十一行 path(\u0026#39;\u0026#39;, include(\u0026#39;game.urls\u0026#39;)), #在第20行之后插入 ","date":"2022-06-27T21:21:26+08:00","permalink":"https://duskandwine.github.io/p/django%E7%AC%94%E8%AE%B0/","title":"Django笔记"},{"content":"obsidian的优点有很多支持Markdown、丰富的插件系统、本地化的资料库、支持同步、最新版本也支持和typora一样的实时预览等等。尽管它是一款优秀的软件，但是还是不要追求all in one，我认为这会导致你记录的东西非常混乱。\n对于我来说obsidian的定位不是信息收集工具(我使用sidenotes和自带的记事本文件)，不是word，不是时间管理工具(sorted3)，当然这些功能都可以在obsidian上实现。obsidian应该是进行对所学知识的管理工具，以及对自己输出的结构化和整理后的想法和换点的整合工具，例如书评影评等。\n人在使用工具的时候，工具也在改变人，obsidian会逼着我们建立知识间的连接，迫使我们把知识单元清晰化。人可以主动选择工具，让工具的逻辑来改变自己做事既有的混乱状态。你在选择工具的时候，工具也会反过来塑造你。当我们再去使用新的工具时，要去想这些新的工具能为我们带来什么新的思考模式或者是新的工作流程，不要为了使用新的东西而去使用新的东西。\n基础配置 文件与链接\n建议打开的核心插件，日记插件可根据资料库情况选择性打开 不同的资料库是无法设置双链的。在建立资料库时要对资料库的定位有清晰的认识。\n每个资料库的配置都是不同的，当建立新的资料库时需要重新配置所有设置。每个资料库的设置存放在该资料库下的.obsidian文件夹下，可以直接复制该文件夹到新资料库以更新配置。\n双链使用 双链是什么具体就不再介绍，这里单介绍一下在obsidian中使用双链的方法。\n链接到文章：[[1.Obsidian简介]]。 链接到某个标题：[[1.Obsidian简介#3 Obsidian定位]]。 链接到文本块(段落)：[[1.Obsidian简介#^e3f585]]。^后的代码是自动生成的，也可以去到原文章该段落末尾以空格+^+xxx的形式自己设定。 链接别名：[[1.Obsidiani简介#^e3f585|Obsidian中文名]]。 使链接的内容展示在当前页面：![[1.Obsidian简介]]。 编辑模式下需要按住control再移动鼠标到双链上查看。\n为什么要使用双链：简单的说双链可以让你的知识产生关联，不是线性的增长。做自己的百科。\n基础技巧 模板 日记功能的模板是独立调用的。在如下面板进行配置： 这样在左侧侧面栏新建日记的时候就会自动调用模板。\n其他文章如果需要使用模板则需要在创建好文档后在侧边栏的插入模板中打开新的模板。注意：应该提前在模板插件中配置模板所放文件夹。 关于模板中可以使用的一些通配符说明：Templates - Obsidian Help\n内容同步 Apple全家桶推荐使用iCloud。直接把所有资料库放在iCloud的中就好了，其他平台的同步方式有百度云坚果云等。 命令面板 mac：command+p/window：ctrl+p调出命令面板。很多第三方插件的功能需要通过命令面板实现。 快捷键 建议深度使用之后再为自己常用的操作设置快捷键。我暂时没有设置快捷键，mac下软件快捷键太多了，不但容易引发冲突还难记。应该为自己系统的全局快捷键和常用软件中的常用快捷键撰写一份文档。 社区插件 推荐插件： 功能复杂插件的说明及教程：\nObsidian 插件之 QuickAdd 使用教程_哔哩哔哩_bilibili\nIntroduction - Templater (silentvoid13.github.io)\n【效率办公】Obsidain插件之Tasks-任务管理利器 - 哔哩哔哩 (bilibili.com)\n","date":"2022-04-13T10:37:03+08:00","image":"https://cdn.jsdelivr.net/gh/filifili233/blogimg@master/uPic/image-20220413120850084.png","permalink":"https://duskandwine.github.io/p/obsidian%E9%85%8D%E7%BD%AE%E5%8F%8A%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/","title":"Obsidian配置及基本用法"},{"content":"数据类型 数值型\n123 #整型\n2.34 #浮点型\n字符型\n\u0026quot;Hello,world\u0026quot; '123' 逻辑型\nTRUE T FALSE 特殊值\nNA #NA表示缺失值，即“Missing value”，是“not available”的缩写 NULL #NA代表位置上的值为空，NULL代表连位置都没有，变量为空，其长度为0，表明“空”。 NaN #Not a Number（非数） Inf #R中的无穷大用Inf表示（即Infinity，无穷大），负无穷表示为-Inf。 -Inf 相关函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #判断数据类型 is.mumeric(123) #判断123是不是数值型，如是返回true is.character(\u0026#39;123\u0026#39;) #判断‘123’是不是字符型 is.logical(FALSE) #判断FALSE是不是逻辑型 #数据类型转换 as.numeric() as.character() as.logical() is.na() is.null() is.nan() if.infinite() 按对象类型来分是以下 6 种 向量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 赋值： # 对象名 \u0026lt;- 对象值，=好也可以赋值，菜鸟教程中说没有区别，但我目前看一些博主说最好用\u0026lt;-进行赋值，=号用来在函数的参数中进行赋值。 #常量 pi #圆周率π letters #小写二十六个英文字母 LETTERS #大写二十六个英文字母 month.name #全拼月份 month.abb #简写月份 #向量赋值 v1 \u0026lt;- 1:5 #v1 int[1:5] 1 2 3 4 5 v2 \u0026lt;- c(v1,3,2,7,4,6) #v2 num[1:10] 1 2 3 4 5 3 2 7 4 6 v3 \u0026lt;- rep(v2, times = 2) #重复两遍第一个参数 v3 num[1:20] 1 2 3 4 5 3 2 7 4 6 1 2 3 4 5 3 2 7 4 6 v4 \u0026lt;- rep(v2, each = 2) #重复第一个参数中的数值每一个两遍 1 1 2 2 3 3 4 4 5 5 3 4 2 2 7 7 4 4 6 6 v5 \u0026lt;- rep(v2, times =2, each = 2) v6 \u0026lt;- seq(from = 2, to = 9, by =2) #取从2开始到9结束的数，每个数差值为2 v7 \u0026lt;- seq(from = 2, to = 9, length.out = 3) #取从2开始到9结束的数,取3个(平均分成两份区域) v8 \u0026lt;- seq(from = 2, by = 3, length.out = 4) #从2开始取值，每个数差值为3，共取四个数 v9 \u0026lt;- c(\u0026#39;aic\u0026#39;, \u0026#39;bic\u0026#39;, \u0026#39;cp\u0026#39;) # c函数只能包含同一类型的数据，如果类型不统一，会进行强制类型转换 # 向量元素名称 names(v2) #1 2 3 4 5 3 2 7 4 6 names(v2) \u0026lt;- v9 v2 #aic bic cp \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; #\t1 2 3 4 5 3 2 7 4 6 names(v2) #\u0026#34;aic\u0026#34; \u0026#34;bic\u0026#34; \u0026#34;cp\u0026#34; NA NA NA NA NA NA NA ","date":"2022-03-27T11:06:48+08:00","permalink":"https://duskandwine.github.io/p/r%E6%95%B0%E6%8D%AE%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/","title":"R数据基本知识"},{"content":"前言 使用homebrew安装一些工具或者软件时，会自动安装相关依赖，这些依赖中包括了很多不同版本的python版本，导致当前mac中的python环境过于混乱，想起之前在服务器上使用过pyenv来管理python版本，感觉还是蛮好用的。\n安装 更新brew\n1 brew update 使用brew安装pyenv\n1 brew install pyenv ​\t配置环境变量并激活\n1 2 3 echo \u0026#39;eval \u0026#34;$(pyenv init -)\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo \u0026#39;eval \u0026#34;$(pyenv virtualenv-init -)\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc pyenv常用命令 1 2 3 4 5 6 7 8 pyenv install --list # 列出可安装版本 pyenv install \u0026lt;version\u0026gt; # 安装对应版本 pyenv install -v \u0026lt;version\u0026gt; # 安装对应版本，若发生错误，可以显示详细的错误信息 pyenv versions # 显示当前使用的python版本 pyenv which python # 显示当前python安装路径 pyenv global \u0026lt;version\u0026gt; # 设置默认Python版本 pyenv local \u0026lt;version\u0026gt; # 当前路径创建一个.python-version, 以后进入这个目录自动切换为该版本 pyenv shell \u0026lt;version\u0026gt; # 当前shell的session中启用某版本，优先级高于global 及 local ","date":"2022-03-26T17:31:49+08:00","permalink":"https://duskandwine.github.io/p/mac%E4%B8%8Bpython%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86/","title":"Mac下python版本管理"},{"content":"从Windows下的hexo到mac 说起来一把辛酸泪，本来以为是很简单的事情，但结果遇到了各种错误。\n先谈谈hexo，我首先用nvm装好了node环境，接着又安装了hexo，随后去blog目录下npm install，完成，之后配置github的时候提示说不再支持账号密码登录，此时可以使用token作为密码和使用ssh方式登录，我选择了ssh方式登录，ssh登录github教程：git配置ssh秘钥（公钥以及私钥）windows - aaa_Eternity味道 - 博客园 (cnblogs.com)，mac下同理，只不过无需使用gitbash，直接打开iterm2即可。\n配置完成后要去hexo配置文件.yml中更改git的连接方式。就在配置好之后hexo d时出现了很多奇怪的错误，经过研究发现是当前的node、hexo的版本过于超前，与之前blog下的配置文件发生冲突，无奈又使用nvm安装较低版本的node，在安装hexo时，应该去blog目录下查看.package文件中指定的hexo版本，再进行安装。\n重新安装一遍之后再hexo d，没有报错，心想终于搞定，但是检查网站的时候发现了问题，我之前使用的评论系统是gittalk，每篇文章下需要手动初始化才可以使用，但是现在无法进行初始化操作，会报错。早有想换评论插件的想法，这不机会来了，经过筛选发现valine评论系统不错支持匿名评论，教程Hexo博客使用valine评论系统无效果及终极解决方案 - 知乎 (zhihu.com)，当我注册号id进行配置时发现我使用的hexo的主题文件中的配置太过老旧，无法支持现在的valine，我想可以升级一下主题来解决这个问题，但是当我找到这主题的主页时该主题的作者已经不再维护了，哭出声，当时选择这个主题只是因为好看，没有考虑到作者的更新频率以及使用人数过少等问题，给今天挖下了大坑。\n从hexo到hugo 本想重新更新一个主题再把sources下的文档直接复制过去就好了，奈何看到了有博主吹hugo，经不住诱惑，所以直接上手尝试，先说结果，确实生成文章的速度飞快，主题在github找了star数量前三的主题，这下应该不怕开发者跑路了哈哈哈。\nhugo最简单的教程：如何搭建Hugo博客并部署到Github page - 知乎 (zhihu.com)。\n以及评论系统的配置：博客 | hugo 博客添加 disqus 评论系统 (360doc.com)。\n值得注意的是push成功之后要在仓库设置页面里找到github pages设置一个主题，随便选一个就可以，不然会404无法访问。\n之后进行了一些文章的转移，因为frontmatter不同所以只能手动操作了。\n觉得每次更新文章手动操作过于繁琐，于是写了一个shell脚本方便自动操作，注意该脚本适合已经至少push成功一次的站点。每次更新会在桌面生成一个log.text，用来记录更新日期和是否成功，如果失败会记录报错内容(我只是监听了两个最容易报错的地方)，对shell不熟悉都是边百度边写，可能写的过于繁琐了，不过好在可以使用。酌情修改位置信息就好了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cd /Users/duskandwine/MyProject/myblog \u0026amp;\u0026amp; hugo if [ $? -ne 0 ]; then echo `date +\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;` faith: \u0026gt;\u0026gt; /Users/duskandwine/Desktop/log.text hugo \u0026gt;\u0026gt; /Users/duskandwine/Desktop/log.text echo ---------------------------------------------- \u0026gt;\u0026gt; /Users/duskandwine/Desktop/log.text else cd /Users/duskandwine/MyProject/myblog/public \u0026amp;\u0026amp; git add . \u0026amp;\u0026amp; git ccommit -m \u0026#34;`date`\u0026#34; if [ $? -ne 0 ]; then echo `date +\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;` faith: \u0026gt;\u0026gt; /Users/duskandwine/Desktop/log.text git commit -m \u0026#34;`date`\u0026#34; \u0026gt;\u0026gt; /Users/duskandwine/Desktop/log.text echo ---------------------------------------------- \u0026gt;\u0026gt; /Users/duskandwine/Desktop/log.text else git push -f --set-upstream origin master \u0026amp;\u0026amp; echo `date +\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;` success! \u0026gt;\u0026gt; /Users/duskandwine/Desktop/log.text fi fi 同样我觉得每次生成新文章也比较麻烦，也写了一个shell脚本，每次只需要输入文章名字就会自动生成文章并打开。\n1 2 3 4 5 6 echo \u0026#34;请输入文件名\u0026#34; read name cd /Users/duskandwine/MyProject/myblog hugo new post/$name.md cd /Users/duskandwine/MyProject/myblog/content/post open $name.md ","date":"2022-03-17T21:02:08+08:00","image":"https://cdn.jsdelivr.net/gh/filifili233/blogimg@master/20220317/wallhaven-y8rvj7.2ljs3lpfg0w0.png","permalink":"https://duskandwine.github.io/p/%E4%BB%8Ewindows%E5%88%B0mac%E4%B8%8B%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E8%B8%A9%E5%9D%91/","title":"从windows到mac下博客迁移踩坑"},{"content":"最近新换了MacBookPro16 m1max，当然少不了一番折腾，从软件到各样的编译环境配置，还是耗费了不少时间。值得一说的是经过两天Safari的体验，最后还是投入edge的怀抱：\n1.Safari的拓展实在是太少了，尽管可以装油猴，但是我有一个浏览网页的使用习惯是看到不理解的文本直接选中拖拽一下会打开新的标签页自动搜索，也就是超级拖拽，奈何Safari没有相关拓展且禁止安装应用商店以外的插件，最后我还是在应用商店找到了一个叫superdrag的拓展，但是收费，咬了咬牙12大洋还是买了，但是根本没用，哭。\n2.打开收藏时切换不同文件夹(我收藏的标签很多都是以文件夹形式分类)时有卡顿，而且不支持右键删除收藏网址，体验非常差。\n3.垂直标签页太香了，用习惯离不开了。\n其余软件部分参考这篇文章：Mac系统、Mac软件的操作和使用技巧整理，这里主要说一下iterm2以及homebrew的安装。\nIterm2 刚换过之后感觉原生的terminal也不错，但是在一些网站被安利了iterm2，体验了一下感觉还不错，diy之后发现比原生终端好看一点。\nIterm2官网：https://iterm2.com 还有汉化版本(v3.4.10)：https://www.macwk.com/soft/iterm2\n进入github克隆这个项目：https://github.com/mbadolato/iTerm2-Color-Schemes，这里的主题也支持原生terminal和windowterminal等等之类，我们需要进入scheme文件夹然后选择自己喜欢自己喜欢的主题，简介中有各种主题的预览，我这里选择了德古拉主题(我的vscode也是这个主题)。\n接下来需要安装一下powerlevel10k，这是美化的关键，github地址：romkatv/powerlevel10k: A Zsh theme (github.com)，跟着文档操作就可以了，如果安装之后想更改设置，可以在终端输入p10k configure重新进行设置。\n之后还可以进行语法补全和语法高亮插件的安装：\n自动补全：https://github.com/zsh-users/zsh-autosuggestions 语法高亮：https://github.com/zsh-users/zsh-syntax-highlighting 这里推荐一个b站up主的视频可以跟着他一步一步操作：\nhomebrew homebrew是mac下一款管安装理软件、环境和各种工具的包，用之前我还有点抗拒，使用过后就是真香。\n官网：The Missing Package Manager for macOS (or Linux) — Homebrew，网站支持中文。\n国内安装的话换源应该可以加快安装速度而且不会报错，我这里使用了代理就直接复制了官网的安装命令，我这里使用的是clashx，直接在菜单里复制终端命令粘贴进终端就可以。测试代理是否生效时不要使用ping，而要使用curl -vv https://www.github.com,因为ping使用的是icmp协议，不支持代理。\nhomebrew常用命令：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 brew –help #查看brew的帮助 brew install git #安装git软件 brew uninstall git #卸载git软件 brew search git #搜索git软件 brew list #显示已经安装软件列表 brew update #更新软件，把所有的Formula目录更新，并且会对本机已经安装并有更新的软件用*标明。 brew upgrade git #更新某具体软件 brew [info | home] [FORMULA…] #查看软件信息 brew cleanup git #单个软件删除，和upgrade一样 brew cleanup #删除所有 brew outdated #查看那些已安装的程序需要更新 brew home * #用浏览器打开 brew info * #显示软件内容信息 brew deps * #显示包依赖 brew config #查看brew配置 安装完成之后使用brew update命令时报错：\n1 2 fatal: Could not resolve HEAD to a revision Already up-to-date. 解决方法：\nbrew update --verbose 打开报错路径：cd /opt/homebrew/Library/Taps/homebrew/homebrew-core 执行：ls -al 执行git fetch --prune origin 执行git pull --rebase origin master 成功后即可执行更新：brew update 这里我在装一些工具的时候会自动下载依赖环境python，导致我系统里python环境紊乱，推荐装一下pyenv来管理python环境。如果遇到切换全局环境失败请参考这篇文章：Mac OS 安装pyenv pyenv切换版本失败 - 简书 (jianshu.com)\n最后记录一下目前我通过brew安装过的工具：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 aom\thugo\tlibvmaf\tpython@3.10 aria2\ticu4c\tlibvorbis\tpython@3.9 autoconf\timath\tlibvpx\trav1e autojump\tisl\tlibx11\treadline bdw-gc\tjpeg\tlibxau\trtmpdump brotli\tjpeg-xl\tlibxcb\trubberband ca-certificates\tlame\tlibxdmcp\tsdl2 cairo\tleptonica\tlibxext\tsnappy cjson\tlibarchive\tlibxrender\tspeex cmocka\tlibass\tlittle-cms2\tsqlite dav1d\tlibb2\tlz4\tsrt ffmpeg\tlibbluray\tlzo\ttcl-tk flac\tlibevent\tm4\ttesseract fontconfig\tlibffi\tmbedtls\tthefuck freetype\tlibidn2\tmpdecimal\ttheora frei0r\tlibmpc\tmpfr\ttmux fribidi\tlibnghttp2\tncurses\ttree gcc\tlibogg\tnettle\tunbound gdbm\tlibpng\tnvm\tutf8proc gettext\tlibpthread-stubs\topencore-amr\twebp giflib\tlibrist\topenexr\twget git\tlibsamplerate\topenjpeg\tx264 git-gui\tlibsndfile\topenssl@1.1\tx265 glib\tlibsodium\topus\txorgproto gmp\tlibsoxr\tp11-kit\txvid gnutls\tlibssh2\tpcre\txz go\tlibtasn1\tpcre2\tyou-get gobject-introspection\tlibtiff\tpixman\tzeromq graphite2\tlibtool\tpkg-config\tzimg guile\tlibunistring\tpyenv\tzstd harfbuzz\tlibvidstab\tpyenv-virtualenv ","date":"2022-03-17T17:35:15+08:00","image":"https://cdn.jsdelivr.net/gh/filifili233/blogimg@master/20220317/wallhaven-k7vjg7.4f2z7m5bi160.jpg","permalink":"https://duskandwine.github.io/p/macbook%E4%B8%8Biterm2%E7%9A%84%E7%BE%8E%E5%8C%96%E5%92%8Chomebrew%E7%9A%84%E5%AE%89%E8%A3%85/","title":"MacBook下iterm2的美化和homebrew的安装"},{"content":"git常用命令： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 1. git教程 Linux基础课的代码托管平台：git.acwing.com 1.1. git基本概念 工作区：仓库的目录。工作区是独立于各个分支的。 暂存区：数据暂时存放的区域，类似于工作区写入版本库前的缓存区。暂存区是独立于各个分支的。 版本库：存放所有已经提交到本地仓库的代码版本 版本结构：树结构，树中每个节点代表一个代码版本。 1.2 git常用命令 git config --global user.name xxx：设置全局用户名，信息记录在~/.gitconfig文件中 git config --global user.email xxx@xxx.com：设置全局邮箱地址，信息记录在~/.gitconfig文件中 git init：将当前目录配置成git仓库，信息记录在隐藏的.git文件夹中 git add XX：将XX文件添加到暂存区 git add .：将所有待加入暂存区的文件加入暂存区 git rm --cached XX：将文件从仓库索引目录中删掉 git commit -m \u0026#34;给自己看的备注信息\u0026#34;：将暂存区的内容提交到当前分支 git status：查看仓库状态 git diff XX：查看XX文件相对于暂存区修改了哪些内容 git log：查看当前分支的所有版本 git reflog：查看HEAD指针的移动历史（包括被回滚的版本） git reset --hard HEAD^ 或 git reset --hard HEAD~：将代码库回滚到上一个版本 git reset --hard HEAD^^：往上回滚两次，以此类推 git reset --hard HEAD~100：往上回滚100个版本 git reset --hard 版本号：回滚到某一特定版本 git checkout — XX或git restore XX：将XX文件尚未加入暂存区的修改全部撤销 git remote add origin git@git.acwing.com:xxx/XXX.git：将本地仓库关联到远程仓库 git push -u (第一次需要-u以后不需要)：将当前分支推送到远程仓库 git push origin branch_name：将本地的某个分支推送到远程仓库 git clone git@git.acwing.com:xxx/XXX.git：将远程仓库XXX下载到当前目录下 git checkout -b branch_name：创建并切换到branch_name这个分支 git branch：查看所有分支和当前所处分支 git checkout branch_name：切换到branch_name这个分支 git merge branch_name：将分支branch_name合并到当前分支上 git branch -d branch_name：删除本地仓库的branch_name分支 git branch branch_name：创建新分支 git push --set-upstream origin branch_name：设置本地的branch_name分支对应远程仓库的branch_name分支 git push -d origin branch_name：删除远程仓库的branch_name分支 git pull：将远程仓库的当前分支与本地仓库的当前分支合并 git pull origin branch_name：将远程仓库的branch_name分支与本地仓库的当前分支合并 git branch --set-upstream-to=origin/branch_name1 branch_name2：将远程的branch_name1分支与本地的branch_name2分支对应 git checkout -t origin/branch_name 将远程的branch_name分支拉取到本地 git stash：将工作区和暂存区中尚未提交的修改存入栈中 git stash apply：将栈顶存储的修改恢复到当前分支，但不删除栈顶元素 git stash drop：删除栈顶存储的修改 git stash pop：将栈顶存储的修改恢复到当前分支，同时删除栈顶元素 git stash list：查看栈中所有元素 git作业： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 (0) 在当前目录下创建文件夹homework，并将homework目录配置成git仓库。后续作业均在homework目录下操作； (1) 创建文件readme.txt，内容包含一行：111； 将修改提交一个commit； (2) 在readme.txt文件末尾新增一行：222； 将修改提交一个commit； (3) 创建文件夹：problem1和problem2； 创建文件problem1/main.cpp。文件内容为下述链接中的代码：https://www.acwing.com/problem/content/submission/code_detail/7834813/； 创建文件problem2/main.cpp。文件内容为下述链接中的代码：https://www.acwing.com/problem/content/submission/code_detail/7834819/； 将修改提交一个commit； (4) 删除文件夹problem2； 创建文件夹problem3； 创建文件problem3/main.cpp。文件内容为下述链接中的代码：https://www.acwing.com/problem/content/submission/code_detail/7834841/； 将readme.txt中最后一行222删掉，然后添加一行333； 将修改提交一个commit； (5) 在https://git.acwing.com/上注册账号并创建仓库，仓库名称为homework； 将本地git仓库上传到AC Git云端仓库； (6) 创建并切换至新分支dev； 在readme.txt文件中添加一行444； 将修改提交一个commit； 将dev分支推送至AC Git远程仓库； (7) 切换回master分支； 在readme.txt文件中添加一行555； 将修改提交一个commit； (8) 将dev分支合并到master分支； 手动处理冲突，使readme文件最终内容包含4行：111、333、555、444； 将修改提交一个commit； (9) 将master分支的版本库push到AC Git云端仓库； 登录myserver服务器（4. ssh作业中配置的服务器）； 创建并清空文件夹：~/homework/lesson_5/； 将AC Git云端仓库clone到~/homework/lesson_5/中； 声明：以上教程内容转载自AcWing中Linux基础课课件，链接：https://www.acwing.com/file_system/file/content/whole/index/content/2932078/，版权所有：y总(闫学灿)~\ngit作业解答： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 homework 0: tmux #养成好习惯在tmux中进行工作 homework 5 create #创建作业5 cd homework/lesson_5 #进入作业文件夹 mkdir homework #创建项目 homework 1: cd homework #进入项目 git init #初始化git仓库 vim readme.txt #创建readme.txt文件并编辑 git add . #将readme.txt添加到暂存区 git commit -m \u0026#34;create readme.txt\u0026#34; #将readme.txt提交到版本库 homework 2: vim readme.txt #添加222 git add . git commit -m \u0026#34;add 222\u0026#34; homework 3: mkdir problem1 problem2 #创建文件夹 cd problem1 #进入problem1目录 vim main.cpp #创建vim文件夹并将代码复制进去 vim ../problem2/main.cpp #同理 git add . git commit -m \u0026#34;create problem1\u0026amp;2 main.cpp\u0026#34; homework 4: rm problem2 -rf #deletedir problem1\u0026amp;problem2 mkdir problem3 #创建文件夹 vim problem3/main.cpp #将代码复制进去 vim readme.txt git add . git commit -m \u0026#34;delete problem2\u0026amp;create problem3mian.cpp\u0026#34; homework 5: git remote add origin git@git.acwing.com:duskandwine/homework.git git push -u origin master homework 6: git checkout -b dev #创建dev分支 git branch #查看所有分支与当前所在分支 vim readme.txt git add . git commit -m \u0026#34;add 444\u0026#34; git push #推一下现有分支，会出现提示根据提示修改命令~ git push --set-upstream origin dev homework 7: git checkout master #切换回master vim readme.txt git add . git commit -m \u0026#34;add 555\u0026#34; homework 8: git merge dev #合并dev分支 vim readme.txt #处理冲突 git add . git commit -m \u0026#34;manage conflict\u0026#34; homework 9: git push #推到云端仓库 ssh myserver #登录到myserver服务器 cd homework mkdir lesson_5 git clone git@git.acwing.com:duskandwine/homework.git #克隆仓库。这里提示我要输入密码，我输入了我账户密码然后一直提示错误，密码也没错，不知道怎么回事，最后还是配置了一下ssh密钥才拉取成功。 end: homework 5 test #测试一下作业正确性 acs@9741645beb99:~$ homework 5 test homework_0 is Right! homework_1 is Right! homework_2 is Right! homework_3 is Right! homework_4 is Right! homework_5 is Right! homework_6 is Right! homework_7 is Right!homework_8 is Right!homework_9 is Right! score: 100/100 #全部正确 ","date":"2021-10-07T01:00:00+02:00","permalink":"https://duskandwine.github.io/p/linux%E4%B9%8Bgit/","title":"Linux之git"},{"content":"1、tmux教程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 功能： (1) 分屏。 (2) 允许断开Terminal连接后，继续运行进程。 结构： 一个tmux可以包含多个session，一个session可以包含多个window，一个window可以包含多个pane。 实例： tmux: session 0: window 0: pane 0 pane 1 pane 2 ... window 1 window 2 ... session 1 session 2 ... 操作： (1) tmux：新建一个session，其中包含一个window，window中包含一个pane，pane里打开了一个shell对话框。 (2) 按下Ctrl + a后手指松开，然后按%：将当前pane左右平分成两个pane。 (3) 按下Ctrl + a后手指松开，然后按\u0026#34;（注意是双引号\u0026#34;）：将当前pane上下平分成两个pane。 (4) Ctrl + d：关闭当前pane；如果当前window的所有pane均已关闭，则自动关闭window；如果当前session的所有window均已关闭，则自动关闭session。 (5) 鼠标点击可以选pane。 (6) 按下ctrl + a后手指松开，然后按方向键：选择相邻的pane。 (7) 鼠标拖动pane之间的分割线，可以调整分割线的位置。 (8) 按住ctrl + a的同时按方向键，可以调整pane之间分割线的位置。 (9) 按下ctrl + a后手指松开，然后按z：将当前pane全屏/取消全屏。 (10) 按下ctrl + a后手指松开，然后按d：挂起当前session。 (11) tmux a：打开之前挂起的session。 (12) 按下ctrl + a后手指松开，然后按s：选择其它session。 方向键 —— 上：选择上一项 session/window/pane 方向键 —— 下：选择下一项 session/window/pane 方向键 —— 右：展开当前项 session/window 方向键 —— 左：闭合当前项 session/window (13) 按下Ctrl + a后手指松开，然后按c：在当前session中创建一个新的window。 (14) 按下Ctrl + a后手指松开，然后按w：选择其他window，操作方法与(12)完全相同。 (15) 按下Ctrl + a后手指松开，然后按PageUp：翻阅当前pane内的内容。 (16) 鼠标滚轮：翻阅当前pane内的内容。 (17) 在tmux中选中文本时，需要按住shift键。（仅支持Windows和Linux，不支持Mac，不过该操作并不是必须的，因此影响不大） (18) tmux中复制/粘贴文本的通用方式： (1) 按下Ctrl + a后松开手指，然后按[ (2) 用鼠标选中文本，被选中的文本会被自动复制到tmux的剪贴板 (3) 按下Ctrl + a后松开手指，然后按]，会将剪贴板中的内容粘贴到光标处 关于以上ctrl+a快捷键的问题，tmux默认快捷键是ctrl+b，该版本的配置文件是y总以前在旷视实习时他的前辈传给他的，ctrl+a更容易按到hhh，当然还有下面的vim也是经过配置的，当然y总把配置文件也给我们了hhh，acwinglinux传家宝了。\n\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\n2、vim教程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 功能： (1) 命令行模式下的文本编辑器。 (2) 根据文件扩展名自动判别编程语言。支持代码缩进、代码高亮等功能。 (3) 使用方式：vim filename 如果已有该文件，则打开它。 如果没有该文件，则打开个一个新的文件，并命名为filename 模式： (1) 一般命令模式 默认模式。命令输入方式：类似于打游戏放技能，按不同字符，即可进行不同操作。可以复制、粘贴、删除文本等。 (2) 编辑模式 在一般命令模式里按下i，会进入编辑模式。 按下ESC会退出编辑模式，返回到一般命令模式。 (3) 命令行模式 在一般命令模式里按下:/?三个字母中的任意一个，会进入命令行模式。命令行在最下面。 可以查找、替换、保存、退出、配置编辑器等。 操作： (1) i：进入编辑模式 (2) ESC：进入一般命令模式 (3) h 或 左箭头键：光标向左移动一个字符 (4) j 或 向下箭头：光标向下移动一个字符 (5) k 或 向上箭头：光标向上移动一个字符 (6) l 或 向右箭头：光标向右移动一个字符 (7) n\u0026lt;Space\u0026gt;：n表示数字，按下数字后再按空格，光标会向右移动这一行的n个字符 (8) 0 或 功能键[Home]：光标移动到本行开头 (9) $或功能键[End]：光标移动到本行末尾 (10) G：光标移动到最后一行 (11) :n 或 nG：n为数字，光标移动到第n行 (12) gg：光标移动到第一行，相当于1G (13) n\u0026lt;Enter\u0026gt;：n为数字，光标向下移动n行 (14) /word：向光标之下寻找第一个值为word的字符串。 (15) ?word：向光标之上寻找第一个值为word的字符串。 (16) n：重复前一个查找操作 (17) N：反向重复前一个查找操作 (18) :n1,n2s/word1/word2/g：n1与n2为数字，在第n1行与n2行之间寻找word1这个字符串，并将该字符串替换为word2 (19) :1,s/word1/word2/g：将全文的word1替换为word2 (20) :1,$s/word1/word2/gc：将全文的word1替换为word2，且在替换前要求用户确认。 (21) v：选中文本 (22) d：删除选中的文本 (23) dd: 删除当前行 (24) y：复制选中的文本 (25) yy: 复制当前行 (26) p: 将复制的数据在光标的下一行/下一个位置粘贴 (27) u：撤销 (28) Ctrl + r：取消撤销 (29) 大于号 \u0026gt;：将选中的文本整体向右缩进一次 (30) 小于号 \u0026lt;：将选中的文本整体向左缩进一次 (31) :w 保存 (32) :w! 强制保存 (33) :q 退出 (34) :q! 强制退出 (35) :wq 保存并退出 (36) :set paste 设置成粘贴模式，取消代码自动缩进 (37) :set nopaste 取消粘贴模式，开启代码自动缩进 (38) :set nu 显示行号 (39) :set nonu 隐藏行号 (40) gg=G：将全文代码格式化 (41) :noh 关闭查找关键词高亮 (42) Ctrl + q：当vim卡死时，可以取消当前正在执行的命令 异常处理： 每次用vim编辑文件时，会自动创建一个.filename.swp的临时文件。 如果打开某个文件时，该文件的swp文件已存在，则会报错。此时解决办法有两种： (1) 找到正在打开该文件的程序，并退出 (2) 直接删掉该swp文件即可 声明：以上教程内容转载自AcWing中Linux基础课课件，链接：https://www.acwing.com/file_system/file/content/whole/index/content/2855620/，版权所有：y总(闫学灿)~\n\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\u0026laquo;\n本课作业 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 (0) 进入homework_0文件夹，创建文件names.txt，并顺次将下列姓名写入该文件，每个名字占一行。 AcWing、yxc、Bob、张强、李明、Alice (1) 进入homework_1文件夹，打开problem.txt，并依次删除下列字符： [1] 最后一行第101个字符 [2] 第3行第8个字符 [3] 第1行第30个字符 [4] 第16行第55个字符 [5] 第9行第80个字符 最后保存文件并退出。 (2) 进入homework_2文件夹，打开problem.txt，并依次执行如下操作： [1] 在第1个\u0026#34;two\u0026#34;的后面添加\u0026#34;abc\u0026#34; [2] 在第2个\u0026#34;two\u0026#34;的前面添加\u0026#34;def\u0026#34; [3] 将第3个\u0026#34;two\u0026#34;后面的连续12个字符删掉 [4] 将第4个\u0026#34;two\u0026#34;所在的行删掉 最后保存文件并退出。 (3) 进入homework_3文件夹，打开problem.txt，并依次执行如下操作： [1] 将第5行至第15行中所有of替换成OF。 [2] 将全文中所有的the替换成THE。 [3] 将第偶数个is替换成IS，第奇数个is不变。下标从1开始。 (4) 进入homework_4文件夹，打开problem.txt，并依次执行如下操作： [1] 删除第11行 [2] 将所删除的行粘贴到文件最后一行的下一行 [3] 复制第5行 [4] 将所复制的行粘贴到文件当前最后一行的下一行 (5) 进入homework_5文件夹，打开problem.txt，并依次执行如下操作： [1] 删除第11行第15个字符（包含该字符）至第13行第5个字符（包含该字符） [2] 将所删除的内容粘贴到文件末尾（注意不要另起一行） [3] 复制第5行第88个字符（包含该字符）至第7行第6个字符（包含该字符） [4] 将所复制的内容粘贴到当前文件末尾（注意不要另起一行） (6) 进入homework_6文件夹，并依次执行如下操作： [1] 清空source0.cpp [2] 将source1.cpp中的第1-3行和第12-24行复制到source0.cpp中 (7) 进入homework_7文件夹，格式化source.cpp (8) 进入homework_8文件夹，打开source.cpp，并依次执行如下操作： [1] 将第15-21行向右缩进2次。 [2] 将第22-23行向左缩进1次。 (9) 进入homework_9文件夹，打开链接：https://www.acwing.com/activity/content/code/content/1694465/ 新建文件source.cpp，将链接中的代码抄进source.cpp文件中。 vim作业解答 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 1. homework 2 create cd homework/lesson_2/homework_0 vim names.txt #i插入 输入完成后按esc退出编辑模式，按：后输入wq保存并推出 homwork 2 test #第一个正确 2. cd ../homework_1 vim problem.txt #按G跳转最后一行，输入101G跳到第101个字符，按i进入编辑模式后删除，按3G跳转第3行……依次完成 homework 2 test 3. cd ../homework_2 vim problem.txt #输入/two查找two，n/N控制上一个下一个，然后在相应的two进行相应的操作，dd为删除该行，输入`:noh` 关闭查找关键词高亮 homework 2 test 4. cd ../homework_3 vim problem.txt #输入`:5,15s/of/OF/g`,输入`1,s/the/THE/g`，输入`1,$s/is/IS/gc` 5. cd ../homework_4 vim problem.txt #11G，yy,dd,G,p,5G,yy,G,p 6. cd ../homework_5 vim problem.txt #11G,15\u0026lt;Space\u0026gt;,←,v,13G,5\u0026lt;Space\u0026gt;,y,11G,15\u0026lt;Space\u0026gt;,←,v,13G,5\u0026lt;Space\u0026gt;,d,G,end,p,5G,88\u0026lt;Space\u0026gt;,v,←,7G,6\u0026lt;Space\u0026gt;,y,G,$,p 7. cd ../homework_6 vim source1.cpp #1G,v,3G,$,y,:wq vim source0.cpp #gg,v,G,$,d,p vim source1.cpp #12G,v,24G,$,y vim source0.cpp #G,$,i,→，回车,esc,p homework 2 test 8. cd ../homework_7 vim source.cpp #gg=G 9. cd ../homework_8 vim source.cpp #21G,\u0026gt;,15G,21G,\u0026gt;,15G,22G,\u0026lt;,23G 10. cd ../homework_9 vim source.cpp #:set paste,粘贴 homework 2 test ","date":"2021-10-01T01:00:00+02:00","permalink":"https://duskandwine.github.io/p/tmux%E5%92%8Cvim/","title":"tmux和vim"},{"content":"R安装 依次安装：\nThe Comprehensive R Archive Network (tsinghua.edu.cn) RStudio | Open source \u0026amp; professional software for data science teams - RStudio 一路是是是就好了。\n包的安装与管理 方式一：直接打开R在菜单栏找到Packages\u0026amp;Data，子菜单中有Package Installer ，打开以后选择CRAN(在线安装)，然后点击Getlist就可以获取所有包的清单了，可以在旁边搜索需要安装的包，在右下部分可以进行安装。当然也可以进行本地安装，但是推荐在线安装，会安装包所需要的相关依赖。 方式二：打开R studio，在Files一栏中找到packages，打开后可以进行包的安装和更新。 方式三：命令行安装：install.packages(\u0026quot;包名\u0026quot;)。 包的加载：library(包名) 单独加载包内的某个函数：包名::函数名 更新包：update.packages()，更新指定包以包名为参数即可，无参数则更新所有包(逐个提示)。 移除包：emove.packages() 基础命令 1 2 3 4 5 6 7 8 9 10 getwd() #获取当前目录 setwd() #设置工作目录 file.choose() #获取文件路径，返回值是路径 read.csv() #读取csv，参数需要是路径 save.image() #保存R.data load() #加载数据，参数需要是路径 data() #加载某个包内置的数据集 ls() #显示当前环境中的对象 rm() #移除某个对象 rm(list = ls()) #移除所有对象 ","date":"2021-03-27T08:41:29+08:00","permalink":"https://duskandwine.github.io/p/r%E8%AF%AD%E8%A8%80%E5%85%A5%E9%97%A8/","title":"R语言入门"},{"content":" 简介：框架就是一个被集成了很多功能且具有很强通用性的一个项目模板。\n是一个专用于异步爬虫的框架。\n高性能的数据解析、请求发送，持久化存储，全站数据爬取，中间件，分布式\u0026hellip; twisted：异步架构\n基本使用\n创建工程：\nscrapy startproject ProName 目录结构：\nspiders：爬虫文件夹 必须存放一个爬虫源文件 settings：工程的配置文件 cd ProName\n创建爬虫源文件：\nscrapy genspider spiderName www.xxx.com 执行工程\nscrapy crawl spiderName 爬虫文件spiderName内容阐述\nname # 爬虫源文件的唯一标识 allowed_domains # 允许的域名 start_urls # 起始的url列表，只可以存储url，列表中存储的url都会被进行get请求 parse # 数据解析 seetings.py：\n禁止robots 指定类型日志 LOG_LEVEL = \u0026lsquo;ERROR\u0026rsquo; UA伪装 scrapy数据解析\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import scrapy class TestSpider(scrapy.Spider): name = \u0026#39;test\u0026#39; # allowed_domains = [\u0026#39;www.xxx.com\u0026#39;] start_urls = [\u0026#39;https://www.qiushibaike.com/text/\u0026#39;] def parse(self, response): div_list = response.xpath(\u0026#39;//*[@id=\u0026#34;content\u0026#34;]/div/div[2]/div\u0026#39;) for div in div_list: # xpath返回的是列表，但列表元素一定是selector类型的对象 # extract可以讲selector对象中data参数存储的字符串提取出来 author = div.xpath(\u0026#39;./div[1]/a[2]/h2/text()\u0026#39;)[0].extract() # 列表调用了extract之后，则表示将列表中每一个selector对象中data对应的字符串提取出来 content = div.xpath(\u0026#39;./a[1]/div/span/text()\u0026#39;).extract() # join转成字符串 content = \u0026#39;\u0026#39;.join(content) print(author, content) break scrapy持久化存储\n基于终端指令：\n要求：只可以将parse方法的返回值存储到本地的文本文件中 scrapy crawl xxx -o filePath 好处：简洁高效边界 缺点：只能存储到指定后缀的文本文件中 基于管道(常用)：\n编码流程： 数据解析 将解析的数据对象封装存储到item类型的对象(item.py) 1 2 3 4 5 class FirstbloodItem(scrapy.Item): # define the fields for your item here like: author = scrapy.Field() content = scrapy.Field() pass 将item类型的对象提交给管道进行持久化存储的操作(spider.py) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def parse(self, response): div_list = response.xpath(\u0026#39;//*[@id=\u0026#34;content\u0026#34;]/div/div[2]/div\u0026#39;) all_data = [] for div in div_list: # xpath返回的是列表，但列表元素一定是selector类型的对象 # extract可以讲selector对象中data参数存储的字符串提取出来 author = div.xpath(\u0026#39;./div[1]/a[2]/h2/text()\u0026#39;)[0].extract() # 列表调用了extract之后，则表示将列表中每一个selector对象中data对应的字符串提取出来 content = div.xpath(\u0026#39;./a[1]/div/span/text()\u0026#39;).extract() # join转成字符串 content = \u0026#39;\u0026#39;.join(content) item = FirstbloodItem() item[\u0026#39;author\u0026#39;] = author item[\u0026#39;content\u0026#39;] = content yield item 在管道类的process_item中要将其接收到的item对象中存储的数据进行持久化存储操作(pipelines.py) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class FirstbloodPipeline: fp = None # 重写父类方法：该方法旨在开始爬虫的时候被调用一次 def open_spider(self, spider): print(\u0026amp;#39;开始\u0026amp;#39;) self.fp = open(\u0026amp;#39;./first.text\u0026amp;#39;, \u0026amp;#39;w\u0026amp;#39;, encoding=\u0026amp;#39;utf-8\u0026amp;#39;) # 用于处理item类型对象 # 该方法可以接收爬虫文件提交过来的item对象 # 该方法每接收到一个item就会被调用一次 def process_item(self, item, spider): author = item[\u0026amp;#39;author\u0026amp;#39;] content = item[\u0026amp;#39;content\u0026amp;#39;] self.fp.write(author+\u0026amp;#39;:\u0026amp;#39;+content+\u0026amp;#39;\\n\u0026amp;#39;) return item def close_spider(self, spider): print(\u0026amp;#39;结束\u0026amp;#39;) self.fp.close() 在配置文件中开启管道 1 2 3 ITEM_PIPELINES = { \u0026#39;FirstBlood.pipelines.FirstbloodPipeline\u0026#39;: 300, # 表示的是优先级，数值越小优先级越高 } 爬虫文件提交的item类型的对象最终会提交给优先级较高的管道类 基于管道将数据添加到数据库\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class mysqlPipeline(object): conn = None cursor = None def open_spider(self, spider): self.conn = pymysql.Connect(host=\u0026#39;localhost\u0026#39;, port=3306, user=\u0026#39;root\u0026#39;, passwd=\u0026#39;newpassword\u0026#39;, db=\u0026#39;fist\u0026#39;, charset=\u0026#39;utf8\u0026#39;) def process_item(self, item, spider): self.cursor = self.conn.cursor() try: insert_sql = \u0026#34;\u0026#34;\u0026#34; insert into first(author, content) VALUES (%s,%s) \u0026#34;\u0026#34;\u0026#34; # 执行插入数据到数据库操作 self.cursor.execute(insert_sql, (item[\u0026#39;author\u0026#39;], item[\u0026#39;content\u0026#39;])) self.conn.commit() except Exception as e: print(e) self.conn.rollback() return item def close_spider(self, spider): self.cursor.close() self.conn.close() settings.py:\n1 2 3 4 ITEM_PIPELINES = { \u0026#39;FirstBlood.pipelines.FirstbloodPipeline\u0026#39;: 300, # 表示的是优先级，数值越小优先级越高 \u0026#39;FirstBlood.pipelines.mysqlPipeline\u0026#39;: 301, # item传递给下一个即将被执行的管道类 } ","date":"2021-03-09T01:00:00+02:00","permalink":"https://duskandwine.github.io/p/scrapy/","title":"scrapy"},{"content":" 概念：基于浏览器自动化的模块\n自动化：可以通过代码指定一些列的行为动作，然后将其作用到浏览器中。\nselenium和爬虫的关联\n便捷的捕获到任意形式动态加载的数据(可见即可得) 实现模拟登录 谷歌驱动下载地址：下载对应版本的驱动\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from selenium import webdriver from time import sleep # 1.基于浏览器的驱动程序实例化一个浏览器对象 bro = webdriver.Chrome(executable_path=\u0026#39;./chromedriver.exe\u0026#39;) # 对目的网站发请求 bro.get(\u0026#39;https://www.jd.com/\u0026#39;) # 定位标签 search_text = bro.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;key\u0026#34;]\u0026#39;) # 像标签中录入数据 search_text.send_keys(\u0026#39;iPhone11\u0026#39;) btn = bro.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;search\u0026#34;]/div/div[2]/button\u0026#39;) btn.click() sleep(5) # 在搜索结果页面进行滚轮向下滑动的操作(执行j操作：js注入) bro.execute_script(\u0026#39;window.scrollTo(0,document.body.scrollHeight)\u0026#39;) sleep(5) bro.quit() 爬取数据\n古诗词网为例：https://www.gushiwen.org/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from selenium import webdriver from time import sleep from lxml import etree url = \u0026#39;https://www.gushiwen.org/\u0026#39; bro = webdriver.Edge(executable_path=\u0026#39;./msedgedriver.exe\u0026#39;) bro.get(url) page_text_list = [] sleep(1) # 捕获当前页面对应的源码 page_text = bro.page_source page_text_list.append(page_text) # 点击下一页 for i in range(2): next_page = bro.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;amore\u0026#34;]\u0026#39;) next_page.click() sleep(1) page_text_list.append(bro.page_source) for page_text in page_text_list: tree = etree.HTML(page_text) li_list = tree.xpath(\u0026#39;/html/body/div[2]/div[1]\u0026#39;) for li in li_list: title = li.xpath(\u0026#39;//p[1]/a/b/text()\u0026#39;) print(title) for page_text in page_text_list: tree = etree.HTML(page_text) n_list = tree.xpath(\u0026#39;/html/body/div[2]/div[1]\u0026#39;) for n in n_list: context = n.xpath(\u0026#39;//div[@class=\u0026#34;contson\u0026#34;]/text()\u0026#39;) print(context) sleep(2) bro.quit() selenium的弊端：\n效率低 动作链Action Chains\n动作链：一系列连续的动作(滑动动作)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from time import sleep from selenium import webdriver from selenium.webdriver import ActionChains url = \u0026#39;https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable\u0026#39; bro = webdriver.Edge(executable_path=\u0026#39;./msedgedriver.exe\u0026#39;) bro.get(url) sleep(1) # 如果通过find系列的函数进行标签定位，如果是存在于iframe下面，则会定位失败 # 解决方案：switch_to bro.switch_to.frame(\u0026#39;iframeResult\u0026#39;) div_tag = bro.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;draggable\u0026#34;]\u0026#39;) action = ActionChains(bro) action.click_and_hold(div_tag) for i in range(6): action.move_by_offset(10, 15).perform() # perform让动作立即执行 sleep(0.5) action.release() bro.quit() 如何让selenium规避检测\n浏览器接管\n找到电脑中安装的谷歌浏览器的驱动程序所在的目录找到。且将目录添加到环境变量中。\n打开cmd，输入：\nchrome.exe \u0026ndash;remote-debugging-port=9222 \u0026ndash;user-data-dir=\u0026ldquo;一个空文件夹的目录\u0026rdquo; 执行如下代码：\n1 2 3 4 5 6 7 8 9 10 from selenium import webdriver from selenium.webdriver.chrome.options import Options chrome_options = Options() chrome_options.add_experimental_option(\u0026#34;debuggerAddress\u0026#34;, \u0026#34;127.0.0.1:9222\u0026#34;) # 本机安装好的谷歌驱动程序路路径 chrome_driver = \u0026#34;C:\\Program Files (x86)\\Gooole\\Chrome\\Application\\chromedriver.exe\u0026#34; driver = webdriver.Chrome(executable_path=chrome_driver, chrome_options=chrome_options) print(driver.title) 指定执行结束后，会打开本机安装好的谷歌浏览器 无头浏览器(无可视化界面浏览器)\nGoogle无头浏览器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from selenium import webdriver from selenium.webdriver.chrome.options import Options import time # 创建一个参数对象，用来控制chrome以无界面模式打开 chrome_options = Options() chrome_options.add_argument(\u0026#39;--headless\u0026#39;) chrome_options.add_argument(\u0026#39;--disable-gpu\u0026#39;) # 驱动路径 path = r\u0026#39;C:\\Users\\ZBLi\\Desktop\\1801\\day05\\ziliao\\chromedriver.exe\u0026#39; # 创建浏览器对象 browser = webdriver.Chrome(executable_path=path, chrome_options=chrome_options) # 上网 url = \u0026#39;http://www.baidu.com/\u0026#39; browser.get(url) time.sleep(3) browser.save_screenshot(\u0026#39;baidu.png\u0026#39;) browser.quit() ","date":"2021-03-07T01:00:00+02:00","permalink":"https://duskandwine.github.io/p/selenium/","title":"SELENIUM"},{"content":" 基于线程池\n线程池\n1 from multiprocessing.dummy import Pool map(callback, list)\n可以使用callback对list中的每一个元素进行指定形式的异步操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import requests import time from multiprocessing.dummy import Pool def get_request(url): page_text = requests.get(url=url).text return len(page_text) # if __name__ == \u0026#34;__main__\u0026#34;: # start = time.time() # urls = [ # \u0026#39;http://127.0.0.1:5000/aoligei\u0026#39;, # \u0026#39;http://127.0.0.1:5000/jay\u0026#39;, # \u0026#39;http://127.0.0.1:5000/haha\u0026#39;, # ] # for url in urls: # res = get_request(url) # print(res) # # print(\u0026#39;耗时\u0026#39;, time.time()-start) # 异步代码 if __name__ == \u0026#34;__main__\u0026#34;: urls = [ \u0026#39;http://127.0.0.1:5000/aoligei\u0026#39;, \u0026#39;http://127.0.0.1:5000/jay\u0026#39;, \u0026#39;http://127.0.0.1:5000/haha\u0026#39;, ] start = time.time() pool = Pool(3) # 3表示开启线程的数量 result_list = pool.map(get_request, urls) # 使用get_request作为回调函数，需要基于异步的形式对urls列表中的没一个列表元素进行操作 回调函数要有一个参数和返回值 print(\u0026#39;耗时\u0026#39;, time.time()-start) 基于单线程+多任务的异步爬虫\n特殊的函数\n如果一个函数的定义被async修饰后，则该函数就变成了一个特殊的函数 该特殊函数调用后，函数内部的实现语句不会被立即执行 该特殊函数被调用后会返回一个协程对象 协程对象\n通过特殊函数的调用返回一个协程对象。 协程 == 一组特定的操作 任务对象\n任务对象就是一个高级协程对象。(是对协程对象的进一步封装) 任务 == 协程 == 特殊函数 == 指定操作 创建一个任务对象 task = asyncio.ensure_future(协程对象) 高级之处：可以给任务对象绑定回调： 任务被执行结束后才可以调用回调函数： 参数(只能有一个)就是该回调函数的调用者(任务对象) result返回的就是特殊函数的返回值 事件循环对象\n作用： 可以将对个任务对象注册到事件循环对象中 如果开启了事件循环后，则其内部注册/装在的任务对象所表示的指定的操作就会被基于异步的被执行 创建方式 loop = asyncio.get_event_loop() /创建事件循环对象 loop.run_until_complete(task) /将任务对象注册到事件循环对象中并开始事件循环 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import requests import asyncio import time import aiohttp urls = [ \u0026#39;http://127.0.0.1:5000/aoligei\u0026#39;, \u0026#39;http://127.0.0.1:5000/jay\u0026#39;, \u0026#39;http://127.0.0.1:5000/haha\u0026#39;, ] async def get_page(url): # requests.get是基于同步，必须使用基于异步的网络请求模块进行指定url的请求发送 # aiohttp：基于异步网络求情的模块 # response = requests.get(url) async with aiohttp.ClientSession() as session: # session. get()/post() # headers,params/data.proxy=\u0026#39;\u0026#39; async with await session.get(url) as response: # text()返回字符串形式的相应数据 # read()返回二进制形式的相应数据 # json()返回json对象 page_text = await response.text() print(page_text) tasks = [] if __name__ == \u0026#39;__main__\u0026#39;: start = time.time() for url in urls: c = get_page(url) task = asyncio.ensure_future(c) tasks.append(task) loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.wait(tasks)) print(time.time() - start) ","date":"2021-03-05T01:00:00+02:00","permalink":"https://duskandwine.github.io/p/%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB/","title":"异步爬虫"},{"content":" cookie\n是存储在客户端的一组键值对。\nweb中cookie的典型应用\n免密登录 登录状态 cookie和爬虫的关系\n对页面请求时，如果请求的过程中不携带cookie，无法请求到正确的页面数据。所以是它是常见的反爬机制。 cookie的处理方式 手动处理 复制粘贴 弊端：有有效时长 自动处理 基于session对象实现自动处理 requests.session（） session对象的作用： 该对象可以像requests一样调用get和post发起指定的请求。只不过如果在使用session发请求的过程中产生了cookie，则cookie会被自动存储在session对象中，那么下次再次使用session请求时，就携带cookie进行了请求。 在使用session的时候，session对象至少会被使用两次。 例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 需求：爬取雪球网中的咨询信息。https://xueqiu.com/ # 分析：判定爬取的数据是否是动态加载：部分数据是动态加载 # 定位到ajax请求的数据包，提取处请求的url，响应数据为json形式的数据 import requests headers = { \u0026#39;User-Agent\u0026#39;: \u0026#39;User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0\u0026#39; } session = requests.session() # 创建session main_url = \u0026#39;https://xueqiu.com/\u0026#39; session.get(main_url, headers=headers) # 捕获且存储cookie url = \u0026#39;https://xueqiu.com/statuses/hot/listV2.json?since_id=-1\u0026amp;max_id=176631\u0026amp;size=15\u0026#39; page_text = session.get(url=url, headers=headers).json() print(page_text) 代理机制\n在爬虫中代理就是代理服务器\n代理服务器的作用是转发请求和响应。\n短时间内对服务器发起了高频的请求，那么服务器会检测到这样的一个异常的行为请求，就会将该请求对应设备的ip禁掉。\n如果ip被禁，则可以使用代理服务器。\n代理服务器不同的匿名程度：\n透明代理：server知道你使用了代理且知道你的真实ip 匿名代理：知道你使用了代理，但是不知道你的真实ip 高匿代理：不知道你是使用了代理也不知道你的真实ip 代理的类型：\nhttps：只能转发https协议的请求 http：转发http的请求 代理服务器：\n快代理\n西祠代理\ngoubanjia\n代理精灵\n封装代理池\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 代理池 from lxml import etree import requests url = \u0026#39;放代理地址\u0026#39; headers = { \u0026#39;User-Agent\u0026#39;: \u0026#39;User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0\u0026#39; } page_text = requests.get(url, headers=headers).text tree = etree.HTML(page_text) proxy_list = tree.xpath(\u0026#39;//body//text()\u0026#39;) http_proxy = [] for proxy in proxy_list: dic = { \u0026#39;http\u0026#39;: proxy } http_proxy.append(dic) 使用代理\n1 2 3 # request = requests.get(url, headers=headers, proxies={\u0026#39;http\u0026#39;:ip:port}) # xpath解析式中不能出现tbody标签 request = requests.get(url, headers=headers, proxies=random.choice(http_proxy)) 验证码识别\n基于线上的打码平台识别验证码\n打码平台\n超级鹰\n创建软件id\n下载示例代码\n封装功能\n1 2 3 4 5 import chaojiying def tanfoormImgCode(path, type): chaojiying1 = chaojiying.Chaojiying_Client(\u0026#39;用户名\u0026#39;, \u0026#39;密码\u0026#39;, \u0026#39;软件id\u0026#39;) im = open(path, \u0026#39;rb\u0026#39;).read() return chaojiying1.PostPic(im, type)[\u0026#39;pic_str\u0026#39;] 云打码\n打码兔\n模拟登录\n流程： 对点击登录按钮对应的请求进行发送(get) 处理请求参数： 用户名 密码 验证码 其他防伪参数 cookie(session) 乱序请求参数(可能动态) 处理方式1：一般动态变化的请求参数会被隐藏在前台页面中，在页面源码中搜索即可。 处理方式2：如果前台页面没有，则机遇抓包工具进行全局搜索。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import requests from lxml import etree from chaojiying import tanfoormImgCode headers = { \u0026#39;User-Agent\u0026#39;: \u0026#39;User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0\u0026#39; } # 识别验证码 session = requests.session() # 创建session捕获cookie url = \u0026#39;https://so.gushiwen.cn/user/login.aspx?from=http://so.gushiwen.cn/user/collect.aspx\u0026#39; page_text = session.get(url, headers=headers).text tree = etree.HTML(page_text) img_src = \u0026#39;https://so.gushiwen.cn\u0026#39; + tree.xpath(\u0026#39;//*[@id=\u0026#34;imgCode\u0026#34;]\u0026#39;)[0] img_data = session.get(img_src, headers=headers).content with open(\u0026#39;./code.jpg\u0026#39;, \u0026#39;wb\u0026#39;) as fp: fp.write(img_data) # 识别验证码 code_text = tanfoormImgCode(\u0026#39;./code.jpg\u0026#39;, 1902) login_url = \u0026#39;https://so.gushiwen.cn/user/login.aspx?from=http%3a%2f%2fso.gushiwen.cn%2fuser%2fcollect.aspx\u0026#39; data = { \u0026#39;__VIEWSTATE\u0026#39;: \u0026#39;9Qg1T/fXvI3ef41DzaaaK7bfCyhDl4c3E3RS1rYNfMx4QF5zKHU43xQ2JKqM2PVNkn228ZRjSIh5F0yXq5gYfAdSZSSBctGUiqunsGgE/NKcajrSMIqiXihWMj0=\u0026#39;, \u0026#39;__VIEWSTATEGENERATOR\u0026#39;: \u0026#39;C93BE1AE\u0026#39;, \u0026#39;from\u0026#39;: \u0026#39;http://so.gushiwen.cn/user/collect.aspx\u0026#39;, \u0026#39;email\u0026#39;: \u0026#39;33192475@qq.com\u0026#39;, \u0026#39;pwd\u0026#39;: \u0026#39;j33192475\u0026#39;, \u0026#39;code\u0026#39;: code_text, # 动态订婚 \u0026#39;denglu\u0026#39;: \u0026#39;登录\u0026#39;, } # 对点击登录发起请求 page_text_login = session.post(login_url, headers=headers, data=data).text print(page_text_login) # TODO jupyter魔法指令 # TODO 正则 # TODO 梨视频爬取 基于百度AI实现的爬虫功能\n图像识别 语音识别\u0026amp;合成 自然语言处理 使用流程： 登录后在相应功能下创建app 选择对应的pythonSDK文档进行代码实现 梨视频爬取思路\n将每一个视频详情页的url进行解析 对视频详情页的url进行请求发送 在视频详情页的页面源码中进行全局搜索，发现没有找到video标签 video是动态加载 动态加载的数据方式 ajax js 通过正则匹配出视频地址 ","date":"2021-03-04T01:00:00+02:00","permalink":"https://duskandwine.github.io/p/cookie-%E4%BB%A3%E7%90%86/","title":"cookie+代理"},{"content":" 使用xpath爬取图片名称和图片数据\n[https://pic.netbian.com/4kmeinv/index.html] 局部数据解析：\n将定位到的页面中的标签作为待解析的数据。 在局部数据时，xpath表达式中要使用./的操作，表示当前的局部数据 需求：要求解析出携带html标签的局部数据？\nbs4，bs4在实现标签定位的时候返回值就是定位到标签对应的字符串数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import os from lxml import etree import requests dirName = \u0026#39;lsp1\u0026#39; if not os.path.exists(dirName): os.mkdir(dirName) headers = { \u0026#39;User-Agent\u0026#39;: \u0026#39;User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0\u0026#39; } main_url = \u0026#39;https://pic.netbian.com/4kmeinv/index.html\u0026#39; # 爬取多页 # 定义一个通用的url模板：不可变 url = \u0026#39;https://pic.netbian.com/4kmeinv/index_%d.html\u0026#39; for page in range(1, 6): if page == 1: new_url = \u0026#39;https://pic.netbian.com/4kmeinv/index.html\u0026#39; else: new_url = format(url % page) response = requests.get(url=new_url, headers=headers) response.encoding = \u0026#39;gbk\u0026#39; page_text = response.text # 图片名称+图片数据 tree = etree.HTML(page_text) # 存储的是定位到的指定的li标签 li_list = tree.xpath(\u0026#39;//div[@class=\u0026#34;slist\u0026#34;]/ul/li\u0026#39;) for li in li_list: # li的数据类型和tree的数据类型一样，li也可以调用xpath方法 title = li.xpath(\u0026#39;./a/img/@alt\u0026#39;)[0]+\u0026#39;.jpg\u0026#39; # [0]表示取字符串 img_src = \u0026#39;https://pic.netbian.com\u0026#39;+li.xpath(\u0026#39;./a/img/@src\u0026#39;)[0] ima_data = requests.get(url=img_src, headers=headers).content imgPath = dirName + \u0026#39;/\u0026#39; + title with open(imgPath, \u0026#39;wb\u0026#39;) as fp: fp.write(ima_data) print(title, \u0026#39;ok!\u0026#39;) xpath表达式如何更具有通用性？\n在xpath表达式中使用管道符分割的作用，两侧表达式同时生效或者一个生效。 ","date":"2021-03-03T01:00:00+02:00","permalink":"https://duskandwine.github.io/p/xpath%E8%A7%A3%E6%9E%90/","title":"xpath解析"},{"content":"bs4 爬取三国全篇内容：《三国演义》全集在线阅读_史书典籍_诗词名句网 (shicimingju.com)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import requests from bs4 import BeautifulSoup headers = { \u0026#39;User-Agent\u0026#39;: \u0026#39;User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0\u0026#39; } main_url = \u0026#39;https://www.shicimingju.com/book/sanguoyanyi.html\u0026#39; page_text = requests.get(url=main_url, headers=headers).text fp = open(\u0026#39;./sanguo.txt\u0026#39;, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) # 数据解析：章节标题，标签页url，章节内容 soup = BeautifulSoup(page_text, \u0026#39;html.parser\u0026#39;) # 定位到所有的符合要求的a标签 a_list = soup.select(\u0026#39;.book-mulu \u0026gt; ul \u0026gt; li \u0026gt; a\u0026#39;) for a in a_list: title = a.string detail_url = \u0026#39;https://www.shicimingju.com\u0026#39; + a[\u0026#39;href\u0026#39;] # 对详情页发起请求解析出章节内容 page_text_detail = requests.get(url=detail_url, headers=headers).text soup = BeautifulSoup(page_text_detail, \u0026#39;html.parser\u0026#39;) div_tag = soup.find(\u0026#39;div\u0026#39;, class_=\u0026#34;chapter_content\u0026#34;) content = div_tag.text fp.write(title+\u0026#39;:\u0026#39;+content+\u0026#39;\\n\u0026#39;) print(title, \u0026#39;保存成功\u0026#39;) fp.close() 注意：新版lxml的命令与之前有所不同 xpath 环境安装：pip install lxml 解析原理：html是以树状的形式进行展示 实例化一个etree的对象，且将待解析的页面源码数据加载到该对象中 调用etree对象的xpath方法结合着不停的xpath表达式实现标签的定位和数据提取。 实例化etree对象 etree.parse(\u0026lsquo;filename\u0026rsquo;)：将本地html文档加载到该对象中 etree.HTML(page_text)：网站获取的页面数据加载到该对象 标签定位 最左侧/：如果xpath表达式最左侧是以/开头则表示该xpath表达式一定要从根标签开始定位指定标签的 非最左侧的/：表示一个层级 非左侧//：表示多个层级 最左侧//：xpath表达式可以从任意位置进行标签定位 属性定位：tagName[@attrNmae=\u0026ldquo;value\u0026rdquo;] 索引定位：tag[index]#索引是从1开始 模糊匹配 取文本 /text()：直系文本内容 //text()：所有文本内容 取属性 /@attrName ","date":"2020-11-30T01:00:00+02:00","permalink":"https://duskandwine.github.io/p/bs4%E5%AE%9E%E6%88%98-xpath%E5%9F%BA%E7%A1%80/","title":"bs4实战+xpath基础"},{"content":"正则 使用正则进行图片数据的批量解析爬取\n如何爬取图片数据\n方式1：基于requests 方式2：基于urllib 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #方式一： import requests import urllib headers = { \u0026#39;User-Agent\u0026#39;: \u0026#39;User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0\u0026#39; } img_url = \u0026#39;https://dss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=2534506313,1688529724\u0026amp;fm=26\u0026amp;gp=0.jpg\u0026#39; response = requests.get(url=img_url, headers=headers) img_data = response.content # content返回的是二进制形式的响应数据 with open(\u0026#39;1.jpg\u0026#39;, \u0026#39;wb\u0026#39;) as fp: fp.write(img_data) #方式二： urllib.request.urlretrieve(img_url, \u0026#39;./2.jpg\u0026#39;) 上述两种爬取图片的不同：\n使用urllib的方式无法进行UA伪装，而requests的方式可以。\n需求：爬取明星图库-校花网 (521609.com)中的图片数据\n分析浏览器工具中elements和network这两个选项卡对应的页面源码呦嗬不同？\nelements中包含的显示的页面源码数据为当前页面所有的数据加载完毕之后对应的完整的页面源码数据。（包含动态加载的数据） network中显示的页面源码数据仅仅为某一个单独的请求对应的响应数据（不包含动态加载的数据） 如果在进行数据解析的时候，一定是需要对页面布局进行分析，如果页面没有动态加载数据则可以直接使用elements对页面布局进行分析，否则只可以使用network对页面数据进行分析。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 dirName = \u0026#39;Img\u0026#39; if not os.path.exists(dirName): os.mkdir(dirName) url = \u0026#39;http://www.521609.com/tuku/\u0026#39; page_text = requests.get(url=url, headers=headers).text # 2.从当前页面解析图片地址 ex = \u0026#39;\u0026lt;li\u0026gt;.*?\u0026lt;img src=\u0026#34;(.*?)\u0026#34; alt.*?\u0026lt;/li\u0026gt;\u0026#39; img_src_list = re.findall(ex, page_text, re.S) for src in img_src_list: src = \u0026#39;http://www.521609.com/\u0026#39; + src imgPath = dirName + \u0026#39;/\u0026#39; + src.split(\u0026#39;/\u0026#39;)[-1] urllib.request.urlretrieve(src, imgPath) print(imgPath, \u0026#39;下载成功！\u0026#39;) 关于split的用法Python split()方法 | 菜鸟教程 (runoob.com)\nbs4 数据解析的作用？\n用来实现聚焦爬虫 网页显示的数据都是存在哪里的？\n存储在html的标签中或标签的属性中 数据解析的原理？\n指定标签的定位，取出标签或标签属性中的数据 bs4解析原理\n实例化一个beautifulsoup的对象，且将待解析的源码数据加载到该对象中 调用beautifulsoup对象中相关方法或者属性进行标签定位和文本数据提取 环境安装：\npip install lxml pip install bs4 beautifulsoup对象的实例化：\nbeautifulsoup(fp,\u0026rsquo;lxml\u0026rsquo;):用来将本地存储的html文档中的数据进行解析 beautifulsoup(page_text,\u0026rsquo;lxml\u0026rsquo;):将互联网上请求到的页面源码数据进行解析 标签定位\nsoup.tagName：只可以定位到第一次出现的tagName标签 soup.find(\u0026rsquo;tagName\u0026rsquo;, attrName=\u0026lsquo;value\u0026rsquo;)：属性定位 soup.findAll()：很find一样用作属性定位，只不过findAll返回的是列表 soup.select(\u0026lsquo;选择器\u0026rsquo;): 类选择器 id选择器 层及选择 \u0026lsquo;大于号\u0026rsquo;:表示一个层级 \u0026lsquo;空格\u0026rsquo;:表示多个层级 取数据\n.text 标签下所有的文本内容 .string 标签下直系的文本内容 取属性\ntag[\u0026lsquo;attrName\u0026rsquo;]\n","date":"2020-11-29T01:00:00+02:00","permalink":"https://duskandwine.github.io/p/%E6%AD%A3%E5%88%99-bs4%E5%9F%BA%E7%A1%80/","title":"正则+bs4基础"},{"content":"如何捕获到动态加载的数据？ 基于抓包工具进行全局搜索 在抓包工具中进行全局搜索 基于抓包工具进行全局搜索不一定可以每次都能定位到动态加载数据对应的数据包。 如果动态加载的数据是经过加密的密文数据，就会搜索不到对应的数据包。(加密解密js混淆) -定位到动态加载数据对应的数据包，从该数据包中就可以提取出\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 url = \u0026#39;https://movie.douban.com/j/chart/top_list\u0026#39; params = { \u0026#39;type\u0026#39;: \u0026#39;24\u0026#39;, \u0026#39;interval_id\u0026#39;: \u0026#39;100:90\u0026#39;, \u0026#39;action\u0026#39;:\u0026#39; \u0026#39;, \u0026#39;start\u0026#39;: \u0026#39;0\u0026#39;, \u0026#39;limit\u0026#39;: \u0026#39;20\u0026#39; } response = requests.get(url=url,params=params,headers=headers) #json()将获取的字符串形式的json数据序列化成字典或者列表对象 page_text = response.json() for movie in page_text: name = movie[\u0026#39;title\u0026#39;] score = movie[\u0026#39;score\u0026#39;] print(name, score) 分页数据的爬取操作 爬取肯德基的餐厅位置数据\nurl：http://www.kfc.com.cn/kfccda/storelist/index.aspx 分析\n在录入关键字的文本框中录入关键字按下搜索按钮，发起的是一个ajax请求 当前页面刷仙出来的位置信息一定是通过ajax请求请求到的数据 基于抓包工具定位到该ajax请求的数据包，从该数据包中捕获到： 请求的url 请求方式 请求携带的参数 看到响应数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #爬取单页 url = \u0026#39;http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword\u0026#39; data = { \u0026#39;cname\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;pid\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;keyword\u0026#39;: \u0026#39;广州\u0026#39;, \u0026#39;pageIndex\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;pageSize\u0026#39;: \u0026#39;10\u0026#39;, } response = requests.post(url=url,data=data,headers=headers) page_text = response.json() for canting in page_text[\u0026#39;Table1\u0026#39;]: title = canting[\u0026#39;storeName\u0026#39;] address = canting[\u0026#39;addressDetail\u0026#39;] print(title, address) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 爬取多页 for page in range(1, 3): url = \u0026#39;http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword\u0026#39; data = { \u0026#39;cname\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;pid\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;keyword\u0026#39;: \u0026#39;广州\u0026#39;, \u0026#39;pageIndex\u0026#39;: str(page), \u0026#39;pageSize\u0026#39;: \u0026#39;10\u0026#39;, } response = requests.post(url=url,data=data,headers=headers) page_text = response.json() for canting in page_text[\u0026#39;Table1\u0026#39;]: title = canting[\u0026#39;storeName\u0026#39;] address = canting[\u0026#39;addressDetail\u0026#39;] print(title, address) 练习 爬取国家药监局前三页企业信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #药监局信息爬取 for page in range(1, 4): url = \u0026#39;http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsList\u0026#39; data = { \u0026#39;on\u0026#39;: \u0026#39;true\u0026#39;, \u0026#39;page\u0026#39;: str(page), \u0026#39;pageSize\u0026#39;: \u0026#39;15\u0026#39;, \u0026#39;productName\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;conditionType\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;applyname\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;applysn\u0026#39;: \u0026#39;\u0026#39;, } response = requests.post(url=url,data=data,headers=headers) page_text = response.json() for shuju in page_text[\u0026#39;list\u0026#39;]: ID = shuju[\u0026#39;ID\u0026#39;] url = \u0026#39;http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsById\u0026#39; data = { \u0026#39;id\u0026#39;: str(ID) } response = requests.post(url=url, data=data, headers=headers) page_text1 = response.json() a = page_text1[\u0026#39;epsName\u0026#39;] b = page_text1[\u0026#39;businessPerson\u0026#39;] print(a, b) ","date":"2020-09-23T01:00:00+02:00","permalink":"https://duskandwine.github.io/p/requests%E5%9F%BA%E7%A1%802/","title":"requests基础2"},{"content":" 爬虫中一个基于网络请求的模块 pip install requests\n作用：模拟浏览器发起请求。\n编码流程：\n1.指定url 2.发起请求 3.获取相应数据 持久化存储 爬取搜狗首页的页面源码数据\n1 2 3 4 5 6 7 8 9 10 import requests #1,指定url url = \u0026#39;https://www.sogou.com/\u0026#39; #2,发起请求 get方法的返回值为响应对象 response = requests.get(url=url) #3,获取响应数据 #.text:返回的是字符串形式的响应数据 page_text = response.text #4,持久化存储 with open(\u0026#39;./sougou.html\u0026#39;, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as fp : fp.write(page_text) 实现一个简易网页采集器\n基于搜狗针对指定不同的关键字将其对应的页面数据进行爬取\n参数动态化：\n如果请求的url携带参数，且我们想要将携带的参数进行动态化操作那么我们必须：\n将携带的动态参数以键值对的形式封装到一个字典中\n将该字典作用到get方法的params参数中即可\n需要将原始携带参数的url中将携带的参数删除\n1 2 3 4 5 6 7 8 9 10 11 12 13 keyWord = input(\u0026#39;输入关键字：\u0026#39;) #携带了请求参数的url，如果想要爬取不同关键字对应的页面，我们需要将url携带的参数进行动态化 #实现参数动态化： params = { \u0026#39;query\u0026#39;: keyWord } url = \u0026#39;https://www.sogou.com/web\u0026#39; #params参数(字典):抱球请求时url携带的参数 response = requests.get(url=url, params=params) page_text = response.text fileName = keyWord+\u0026#39;.html\u0026#39; with open(fileName, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as fp: fp.write(page_text) print(fileName, \u0026#39;爬取完毕!\u0026#39;) 上述简易采集器出现的问题：\n乱码 数据丢失 解决乱码：\n修改响应数据的编码格式\nencoding返回的是相应数据的原始的编码格式，如果给其赋值则表示修改了相应数据的编码格式\n在第9行后插入response.encoding = 'utf-8'\n处理乱码后，页面显示[异常访问请求]导致数据丢失。\n异常的访问请求 网站后台检测出异常。 网站的后台是如何知道请求是否是通过浏览器发起的？ 通关判定请求头中的user-agent 什么是user-agent\n请求载体(爬虫，浏览器)的身份标识。浏览器的身份标识是统一固定的，身份标识可以从抓包工具中获取。爬虫程序的身份标识是各自不同的。 第二种反爬机制：\nUA检测：网站后台会检测请求对应的user-agent，以判定当前请求是否为异常请求 反反爬策略：\nUA伪装：被作用到了大部分网站中 伪装流程： 从抓包工具中捕获到某一个基于浏览器请求的user-agent的值，将其伪装作用到一个字典中，将该字典作用到请求方法(get,post)的headers参数中即可。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 keyWord = input(\u0026#39;输入关键字：\u0026#39;) headers = { \u0026#39;User-Agent\u0026#39;:\u0026#39;User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0\u0026#39; } #携带了请求参数的url，如果想要爬取不同关键字对应的页面，我们需要将url携带的参数进行动态化 #实现参数动态化： params = { \u0026#39;query\u0026#39;: keyWord } url = \u0026#39;https://www.sogou.com/web\u0026#39; #params参数(字典):抱球请求时url携带的参数 response = requests.get(url=url, params=params,headers=headers) #修改响应数据的编码格式 #encoding返回的是相应数据的原始的编码格式，如果给其赋值则表示修改了相应数据的编码格式 response.encoding = \u0026#39;utf-8\u0026#39; page_text = response.text fileName = keyWord+\u0026#39;.html\u0026#39; with open(fileName, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as fp: fp.write(page_text) print(fileName, \u0026#39;爬取完毕!\u0026#39;) 爬取豆瓣电影中的电影详情数据\nurl：https://movie.douban.com/typerank?type_name=%E5%96%9C%E5%89%A7\u0026amp;type=24\u0026amp;interval_id=100:90\u0026amp;action=\n动态加载数据的捕获\n什么叫做动态加载的数据？ 我们通过requests模块进行数据爬取无法每次都实现可见即可得。因为有些数据是通过非浏览器地址栏中的url请求到的数据，而是其他请求请求到的数据，那么这些通过其他请求请求到的数据就是动态加载的数据。 如何检测网页中是否存在动态加载数据。\n基于抓包工具进行局部搜索。 在当前网页中打开抓包工具，捕获到地址栏的url对应的数据包，在该数据包的response选项卡搜索我们想要爬取的数据，如果搜索到了结果则表示数据不是动态加载的我，否则表示数据为动态加载的。 ","date":"2020-09-19T01:00:00+02:00","permalink":"https://duskandwine.github.io/p/requests%E5%9F%BA%E7%A1%801/","title":"requests基础1"},{"content":"jupyter 的基本使用 new新建：\n由cell组成：cell就是一行可编辑框\ncell的作用：\n根据不同的模式和笔记进行代码和笔记的编写，编写好的代码和笔记可以直接在当前文件中运行，并看到运行结果。 cell模式：\ncode： 可以编写pyhon代码，可以编写一行或多行。 特性：编写代码的顺序是无所谓的，但是代码执行的顺序一定是自上而下的。 markdown：编写笔记 folder：新建文件夹\ntext file：新建一个任意后缀的文本文件：\n可以写程序，但是不能在文件中执行。 terminal：新建一个基于浏览器的终端。\n快捷键的使用：\n插入cell：a，b 删除cell：x 执行cell：shift+enter 切换cell的模式：m，y cell执行后，在cell左侧双击可以回到cell的可编辑模式 执行结果的收回：在执行结果左侧双击即可 打开帮助文档：shift+tab tab：自动补全 撤回操作：z jupyter的源文件导出：\nfile\u0026ndash;\u0026gt;download as\u0026ndash;\u0026gt;html 爬虫概述 什么是爬虫？ 就是通过编写程序，让其模拟浏览器上网，然后再互联网中抓全数据的过程。 关键词抽取： 模拟：浏览器就是一个纯天然最原始的一个爬虫工具。 抓取： 抓取一整张的页面源码数据 抓取一整张页面的中的局部数据 爬虫的分类： 通用爬虫： 要求我们爬取一整张页面源码数据 聚焦爬虫： 要求爬取一张页面中的局部的数据 聚焦爬虫一定是建立在通用爬虫的基础之上的。 增量式爬虫： 用来监测网站数据更新的情况，一遍爬取到网站最新更新出来的数据。 分布式爬虫： 提高爬取效率的终极武器。 反爬机制： 是作用到门户网站中。如果网站不想让爬虫轻易爬取到数据，它可以制定相关的机制或措施组织爬虫程序爬取其数据。 反反爬策略： 是作用在爬虫程序中。我们爬虫可以制定相关的策略破极反爬机制从而爬取到相关的数据。 第一个反爬机制 robots协议：(君子协议- -) 是一个纯文本的协议，协议中规定了该网站中哪些数据可以被爬虫爬取，哪些不可以被爬取。 破解： 主观性的不遵从该协议即可。 ","date":"2020-09-11T01:00:00+02:00","permalink":"https://duskandwine.github.io/p/%E7%88%AC%E8%99%AB%E6%A6%82%E8%BF%B0/","title":"爬虫概述"}]